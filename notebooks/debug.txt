# 代码功能测试

关于 attentions.py
    MultiHeadAttention
        _relative_position_to_absolute_position(...)
>>> l = 5  # 假设 length=5
>>> a = torch.tensor([i for i in range(l*(2*l-1))]).view([l, 2*l-1])
>>> a
tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],
        [ 9, 10, 11, 12, 13, 14, 15, 16, 17],
        [18, 19, 20, 21, 22, 23, 24, 25, 26],
        [27, 28, 29, 30, 31, 32, 33, 34, 35],
        [36, 37, 38, 39, 40, 41, 42, 43, 44]])
>>> a.shape
torch.Size([5, 9])
>>> a = torch.nn.functional.pad(a, [0,1,0,0])
>>> a
tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  0],
        [ 9, 10, 11, 12, 13, 14, 15, 16, 17,  0],
        [18, 19, 20, 21, 22, 23, 24, 25, 26,  0],
        [27, 28, 29, 30, 31, 32, 33, 34, 35,  0],
        [36, 37, 38, 39, 40, 41, 42, 43, 44,  0]])
>>> b = a.view(l*2*l)
>>> b.shape
torch.Size([50])
>>> c = torch.nn.functional.pad(b, [0, l-1])
>>> c.shape
torch.Size([54])
>>> d = c.view([l+1, 2*l-1])
>>> d
tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],
        [ 0,  9, 10, 11, 12, 13, 14, 15, 16],
        [17,  0, 18, 19, 20, 21, 22, 23, 24],
        [25, 26,  0, 27, 28, 29, 30, 31, 32],
        [33, 34, 35,  0, 36, 37, 38, 39, 40],
        [41, 42, 43, 44,  0,  0,  0,  0,  0]])
>>> e = d[:l, l-1:]
>>> e  # e 相当于取了 d 右上角的非 pad 部分
tensor([[ 4,  5,  6,  7,  8],
        [12, 13, 14, 15, 16],
        [20, 21, 22, 23, 24],
        [28, 29, 30, 31, 32],
        [36, 37, 38, 39, 40]])

_absolute_position_to_relative_position

>>> f = torch.nn.functional.pad(e, [0, l-1, 0, 0])
>>> f
tensor([[ 4,  5,  6,  7,  8,  0,  0,  0,  0],
        [12, 13, 14, 15, 16,  0,  0,  0,  0],
        [20, 21, 22, 23, 24,  0,  0,  0,  0],
        [28, 29, 30, 31, 32,  0,  0,  0,  0],
        [36, 37, 38, 39, 40,  0,  0,  0,  0]])
>>> g = f.view([l*l+l*(l-1)])
>>> g
tensor([ 4,  5,  6,  7,  8,  0,  0,  0,  0, 12, 13, 14, 15, 16,  0,  0,  0,  0,
        20, 21, 22, 23, 24,  0,  0,  0,  0, 28, 29, 30, 31, 32,  0,  0,  0,  0,
        36, 37, 38, 39, 40,  0,  0,  0,  0])
>>> h = torch.nn.functional.pad(g, [l, 0])
>>> i = h.view(l, 2*l)
>>> i
tensor([[ 0,  0,  0,  0,  0,  4,  5,  6,  7,  8],
        [ 0,  0,  0,  0, 12, 13, 14, 15, 16,  0],
        [ 0,  0,  0, 20, 21, 22, 23, 24,  0,  0],
        [ 0,  0, 28, 29, 30, 31, 32,  0,  0,  0],
        [ 0, 36, 37, 38, 39, 40,  0,  0,  0,  0]])
>>> j = i[:, 1:]
>>> j
tensor([[ 0,  0,  0,  0,  4,  5,  6,  7,  8],
        [ 0,  0,  0, 12, 13, 14, 15, 16,  0],
        [ 0,  0, 20, 21, 22, 23, 24,  0,  0],
        [ 0, 28, 29, 30, 31, 32,  0,  0,  0],
        [36, 37, 38, 39, 40,  0,  0,  0,  0]])
>>>